{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import Any, Union, Optional, Callable\n",
    "from pydantic import SecretStr\n",
    "from urllib.error import HTTPError\n",
    "import time\n",
    "from Bio import Entrez\n",
    "from tqdm.auto import tqdm\n",
    "import feedparser\n",
    "import fitz\n",
    "import requests\n",
    "import hashlib\n",
    "from newspaper import Article\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_core.documents import Document\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import Distance, VectorParams, SparseVectorParams\n",
    "from langchain_qdrant import QdrantVectorStore, FastEmbedSparse, RetrievalMode\n",
    "# import os\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b488c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtractionModule():\n",
    "    \"\"\"\n",
    "    Fetches and processes recent articles from PubMed Central (PMC).\n",
    "\n",
    "    This method retrieves metadata and full-text content for research papers indexed in PMC, \n",
    "    within the time window specified by the module's `lookup_window_size`. It handles batching, \n",
    "    rate limiting, and error retries. Articles that have already been ingested (based on the \n",
    "    provided `ingested_logs`) are skipped.\n",
    "\n",
    "    For each fetched article, the following information is extracted and returned as a dictionary:\n",
    "        - doc_id: Unique identifier for the document (PMC ID).\n",
    "        - source: Source label (\"PubMed Central\").\n",
    "        - doc_type: Document type (\"research_paper\").\n",
    "        - title: Cleaned article title.\n",
    "        - abstract: Cleaned abstract text.\n",
    "        - body_text: Cleaned body text (or abstract if body is missing).\n",
    "        - published_date: ISO 8601 formatted publication date, if available.\n",
    "        - published_date_ts: Publication timestamp (float), or 0.0 if unavailable.\n",
    "        - url: Direct URL to the article on PMC.\n",
    "\n",
    "    Returns:\n",
    "        List[dict[str, Any]]: A list of dictionaries, each representing a parsed PMC research article.\n",
    "\n",
    "    Notes:\n",
    "        - Articles outside the specified date window or already ingested are excluded.\n",
    "        - Handles HTTP errors, rate limiting (429), and retries failed fetches up to a maximum.\n",
    "        - Abstracts and bodies are recursively extracted and HTML-cleaned.\n",
    "        - Uses the NCBI Entrez API with the provided API key.\n",
    "        - Progress is displayed using tqdm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        ncbi_api_key: Union[SecretStr, str],\n",
    "        ingested_logs: set[str],\n",
    "        lookup_window_size: int = 90 # (in days)\n",
    "    ):\n",
    "        if isinstance(ncbi_api_key, str):\n",
    "            ncbi_api_key = SecretStr(ncbi_api_key)\n",
    "        \n",
    "        self.ncbi_api_key = ncbi_api_key\n",
    "        \n",
    "        self.lookup_window_size = lookup_window_size\n",
    "        \n",
    "        self.cutoff = self._days_ago(self.lookup_window_size)\n",
    "        self.ingested: set[str] = ingested_logs\n",
    "        \n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                          \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                          \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "        })\n",
    "        \n",
    "    def fetch_pmc(self) -> list[dict[str, Any]]:\n",
    "        RETMAX: int = 2000 # currently small value for prototyping\n",
    "        BATCH_SIZE: int = 400\n",
    "        THROTTLE_SECONDS: float = 6.0\n",
    "        MAX_RETRIES: int = 3\n",
    "            \n",
    "        def _get_pmcid(article_id_list: list[Any]) -> Optional[str]:\n",
    "            for elem in article_id_list:\n",
    "                id_str = str(elem)\n",
    "                id_type = getattr(elem, \"attributes\", {}).get(\"pub-id-type\", \"\")\n",
    "                if id_type == 'pmcid':\n",
    "                    return id_str\n",
    "            return None\n",
    "\n",
    "        def _extract_abstract(dict_elements: Union[dict, list[dict]], max_depth: Optional[int] = None, _depth: int = 0) -> str:\n",
    "            if not isinstance(dict_elements, list):\n",
    "                dict_elements = [dict_elements]\n",
    "\n",
    "            paragraphs: list[str] = []\n",
    "\n",
    "            for elem in dict_elements:\n",
    "                if not isinstance(elem, dict):\n",
    "                    continue\n",
    "\n",
    "                p_texts = elem.get(\"p\", [])\n",
    "                if isinstance(p_texts, str):\n",
    "                    paragraphs.append(p_texts.strip())\n",
    "                elif isinstance(p_texts, list):\n",
    "                    for p in p_texts:\n",
    "                        if isinstance(p, str):\n",
    "                            paragraphs.append(p.strip())\n",
    "\n",
    "                if max_depth is None or _depth < max_depth:\n",
    "                    for sec in elem.get(\"sec\", []):\n",
    "                        nested = _extract_abstract(sec, max_depth, _depth + 1)\n",
    "                        if nested:\n",
    "                            paragraphs.append(nested)\n",
    "\n",
    "            raw_abstract = \"\\n\\n\".join(p for p in paragraphs if p)\n",
    "            cleaned = BeautifulSoup(raw_abstract, \"html.parser\").get_text(separator=\" \")\n",
    "            return \" \".join(cleaned.split())\n",
    "\n",
    "        def _parse_pub_date(pub_date_elements, preferred_types: tuple = ('epub','ppub','collection')) -> Union[datetime.datetime, None]:\n",
    "            best = None\n",
    "\n",
    "            for elem in pub_date_elements or []:\n",
    "                attrs = getattr(elem, 'attributes', None)\n",
    "                pub_type = ''\n",
    "                if isinstance(attrs, dict):\n",
    "                    pub_type = attrs.get('pub-type','').lower()\n",
    "                if pub_type not in preferred_types:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    parts = [int(x) for x in list(elem)]\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                if len(parts) == 3:\n",
    "                    day, month, year = parts\n",
    "                elif len(parts) == 2:\n",
    "                    day = 1\n",
    "                    month, year = parts\n",
    "                elif len(parts) == 1:\n",
    "                    day = 1\n",
    "                    month = 1\n",
    "                    year = parts[0]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    dt = datetime.datetime(year, month, day)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                idx = preferred_types.index(pub_type)\n",
    "                precision = len(parts)\n",
    "\n",
    "                if (best is None or idx < best[1] or (idx == best[1] and precision > best[2])):\n",
    "                    best = (dt, idx, precision)\n",
    "\n",
    "            return best[0] if best else None\n",
    "\n",
    "        def _extract_body_text(elems: Union[dict, list[dict]], max_depth: Optional[int] = None, _depth: int = 0) -> str:\n",
    "            if not isinstance(elems, list):\n",
    "                elems = [elems]\n",
    "\n",
    "            paragraphs: list[str] = []\n",
    "\n",
    "            for elem in elems:\n",
    "                if not isinstance(elem, dict):\n",
    "                    continue\n",
    "\n",
    "                p = elem.get(\"p\", [])\n",
    "                if isinstance(p, str):\n",
    "                    paragraphs.append(p.strip())\n",
    "                elif isinstance(p, list):\n",
    "                    for text in p:\n",
    "                        if isinstance(text, str):\n",
    "                            paragraphs.append(text.strip())\n",
    "\n",
    "                if max_depth is None or _depth < max_depth:\n",
    "                    for child_sec in elem.get(\"sec\", []):\n",
    "                        extracted = _extract_body_text(child_sec, max_depth, _depth + 1)\n",
    "                        if extracted:\n",
    "                            paragraphs.append(extracted)\n",
    "\n",
    "            return \"\\n\\n\".join([para for para in paragraphs if para])\n",
    "        \n",
    "        docs: list[dict[str, Any]] = []\n",
    "        Entrez.email = 'vivalaraza234@gmail.com'\n",
    "        Entrez.api_key = self.ncbi_api_key.get_secret_value()\n",
    "        \n",
    "        try:\n",
    "            search_handle = Entrez.esearch(\n",
    "                db=\"pmc\",\n",
    "                term=\"all[sb]\",\n",
    "                mindate=self.cutoff.date().isoformat(),\n",
    "                retmax=RETMAX\n",
    "            )\n",
    "            record = Entrez.read(search_handle, validate=False)\n",
    "            search_handle.close()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Entrez.esearch failed: {e}\")\n",
    "            return []\n",
    "\n",
    "        id_list = record.get(\"IdList\", [])\n",
    "        total_ids = len(id_list)\n",
    "        if total_ids == 0:\n",
    "            return []\n",
    "\n",
    "        pbar = tqdm(total=total_ids, desc=\"PMC docs processed\", unit=\"id\")\n",
    "        \n",
    "        for i in range(0, total_ids, BATCH_SIZE):\n",
    "            batch_ids = id_list[i: i + BATCH_SIZE]\n",
    "            articles = []\n",
    "\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    fetch_handle = Entrez.efetch(\n",
    "                        db=\"pmc\",\n",
    "                        id=\",\".join(batch_ids),\n",
    "                        rettype=\"full\",\n",
    "                        retmode=\"xml\"\n",
    "                    )\n",
    "                    articles = Entrez.read(fetch_handle, validate=False)\n",
    "                    fetch_handle.close()\n",
    "                    break\n",
    "                except HTTPError as e:\n",
    "                    if e.code == 429:\n",
    "                        print(\"[WARN] Rate limit hit (429). Sleeping 60s before retry…\")\n",
    "                        time.sleep(60)\n",
    "                    else:\n",
    "                        print(f\"[ERROR] Entrez.efetch HTTPError: {e}. Skipping this batch.\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Entrez.efetch failed (attempt {attempt+1}): {e}. Retrying in 3s…\")\n",
    "                    time.sleep(3)\n",
    "            else:\n",
    "                print(\"[ERROR] All retries for this batch failed. Skipping batch.\")\n",
    "                pbar.update(len(batch_ids))\n",
    "                continue\n",
    "\n",
    "            for art in articles:\n",
    "                try:\n",
    "                    pmc_id = _get_pmcid(art.get(\"front\", {}).get(\"article-meta\", {}).get(\"article-id\", []))\n",
    "                    if not pmc_id:\n",
    "                        continue\n",
    "                        \n",
    "                    doc_id = f\"PMC_{pmc_id}\"\n",
    "                    if doc_id in self.ingested:\n",
    "                        continue\n",
    " \n",
    "                    front = art.get(\"front\", {}).get(\"article-meta\", {})\n",
    "                    pubdate  = _parse_pub_date(front.get(\"pub-date\", []))\n",
    "                    \n",
    "                    raw_title    = front.get(\"title-group\", {}).get(\"article-title\", \"\")\n",
    "                    raw_abstract = _extract_abstract(front.get(\"abstract\", []), max_depth=1)\n",
    "                    raw_body     = _extract_body_text(art.get(\"body\", {}), max_depth=100)\n",
    "\n",
    "                    title    = self._clean_html(raw_title)\n",
    "                    abstract = self._clean_html(raw_abstract)\n",
    "                    body = self._clean_html(raw_body) or abstract\n",
    "\n",
    "                    docs.append({\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"source\": \"PubMed Central\",\n",
    "                        \"doc_type\": \"research_paper\",\n",
    "                        \"title\": title,\n",
    "                        \"abstract\": abstract,\n",
    "                        \"body_text\": body or abstract,\n",
    "                        \"published_date\": None if not pubdate else pubdate.isoformat(),\n",
    "                        \"published_date_ts\": 0.0 if not pubdate else float(pubdate.timestamp()),\n",
    "                        \"url\": f\"https://pmc.ncbi.nlm.nih.gov/articles/{pmc_id}/\"\n",
    "                    })\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Failed parsing article PMC_{pmc_id}: {e}. Skipping it.\")\n",
    "                    continue\n",
    "\n",
    "            time.sleep(THROTTLE_SECONDS)\n",
    "\n",
    "        pbar.close()\n",
    "        return docs\n",
    "    \n",
    "    def fetch_arxiv(self, categories: list[str] = None, per_category_pdf_cap: int = 300) -> list[dict[str, Any]]:\n",
    "        categories = categories or [\"q-bio.*\", \"physics.bio-ph\", \"physics.med-ph\"] # excluded {\"cs.CV\", \"cs.LG\", \"cs.AI\"} for prototype\n",
    "        BASE = \"http://export.arxiv.org/api/query?\"\n",
    "        ITEMS_PER_PAGE = 200\n",
    "        MAX_RETRIES = 3\n",
    "        THROTTLE_SECONDS = 3 # arXiv has 1 req/3s rate limit\n",
    "\n",
    "        def _extract_pdf_text(pdf_url: str) -> str:\n",
    "            try:\n",
    "                pdf_bytes = self.session.get(pdf_url, timeout=20).content\n",
    "                with fitz.open(stream=pdf_bytes, filetype=\"pdf\") as doc:\n",
    "                    return \"\\n\\n\".join(p.get_text() for p in doc)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] PDF parse failed ({pdf_url}): {e}\")\n",
    "                return \"\"\n",
    "\n",
    "        all_docs: list[dict[str, Any]] = []\n",
    "\n",
    "        for cat in categories:\n",
    "            kept: list[dict[str, Any]] = []\n",
    "            start = 0\n",
    "\n",
    "            pbar = tqdm(total=per_category_pdf_cap, desc=f\"arXiv {cat:<8} processed\", unit=\"pdf\")\n",
    "\n",
    "            while len(kept) < per_category_pdf_cap:\n",
    "                url = (\n",
    "                    f\"{BASE}search_query=cat:{cat}\"\n",
    "                    f\"&sortBy=submittedDate&sortOrder=descending\"\n",
    "                    f\"&start={start}&max_results={ITEMS_PER_PAGE}\"\n",
    "                )\n",
    "\n",
    "                feed = None\n",
    "                for attempt in range(MAX_RETRIES):\n",
    "                    try:\n",
    "                        raw = self.session.get(url, timeout=10).text\n",
    "                        feed = feedparser.parse(raw)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"[WARN] arXiv fetch fail (try {attempt+1}): {e}\")\n",
    "                        time.sleep(THROTTLE_SECONDS)\n",
    "                if not feed or not feed.entries:\n",
    "                    time.sleep(THROTTLE_SECONDS)\n",
    "                    break\n",
    "\n",
    "                stop_early = False\n",
    "                for entry in feed.entries:\n",
    "                    pub_dt = datetime.datetime(*entry.published_parsed[:6])\n",
    "                    if pub_dt < self.cutoff:\n",
    "                        stop_early = True\n",
    "                        break\n",
    "\n",
    "                    arxiv_id = entry.id.rsplit(\"/\", 1)[-1]\n",
    "                    doc_id   = f\"ARXIV_{arxiv_id}\"\n",
    "                    if doc_id in self.ingested:\n",
    "                        continue\n",
    "\n",
    "                    pdf_url = next(\n",
    "                        (lnk.href for lnk in entry.links if lnk.rel == \"related\" and lnk.type == \"application/pdf\"),\n",
    "                        None\n",
    "                    )\n",
    "                    if not pdf_url:\n",
    "                        print(f\"[WARN] No PDF URL found for arXiv entry with id {arxiv_id}, skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    body = _extract_pdf_text(pdf_url)\n",
    "                    if not body.strip():\n",
    "                        continue\n",
    "\n",
    "                    kept.append({\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"source\": \"arXiv\",\n",
    "                        \"doc_type\": \"research_paper\",\n",
    "                        \"title\": entry.title,\n",
    "                        \"abstract\": entry.summary,\n",
    "                        \"body_text\": body,\n",
    "                        \"published_date\": None if not pub_dt else pub_dt.isoformat(),\n",
    "                        \"published_date_ts\": 0.0 if not pub_dt else float(pub_dt.timestamp()),\n",
    "                        \"url\": entry.link,\n",
    "                        \"metadata\": {\n",
    "                            \"authors\": [a.name for a in entry.authors],\n",
    "                            \"category\": cat\n",
    "                        },\n",
    "                    })\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    if len(kept) >= per_category_pdf_cap:\n",
    "                        break\n",
    "                    time.sleep(THROTTLE_SECONDS)\n",
    "\n",
    "                if stop_early or len(feed.entries) < ITEMS_PER_PAGE:\n",
    "                    break\n",
    "                start += ITEMS_PER_PAGE\n",
    "\n",
    "            pbar.close()\n",
    "            all_docs.extend(kept)\n",
    "\n",
    "        return all_docs\n",
    "    \n",
    "    def fetch_clinical_trials(self) -> list[dict[str, Any]]:\n",
    "        BATCH_SIZE = 1000  # max allowed is 1000; currently small value for prototype\n",
    "        THROTTLE_SECONDS = 1\n",
    "        MAX_RETRIES = 3\n",
    "        RETMAX = 10000\n",
    "\n",
    "        def _parse_date(text: str) -> Optional[datetime.datetime]:\n",
    "            try:\n",
    "                return datetime.datetime.fromisoformat(text)\n",
    "            except Exception:\n",
    "                return None\n",
    "            \n",
    "        def _clean_html(text: str) -> str:\n",
    "            soup = BeautifulSoup(text or \"\", \"html.parser\")\n",
    "            return \" \".join(soup.get_text(separator=\" \").split())\n",
    "\n",
    "        kept_docs = []\n",
    "        next_page_token = None\n",
    "\n",
    "        pbar = tqdm(total=RETMAX, desc=\"CT.gov trials processed\", unit=\"trial\")\n",
    "        \n",
    "        today = datetime.datetime.now(datetime.UTC).date()\n",
    "        expr = f\"AREA[LastUpdatePostDate]RANGE[{self.cutoff.date()},{today}]\"\n",
    "\n",
    "        while len(kept_docs) < RETMAX:\n",
    "            url = (\n",
    "                \"https://clinicaltrials.gov/api/v2/studies?\"\n",
    "                f\"query.term={expr}\"\n",
    "                f\"&format=json\"\n",
    "                f\"&pageSize={BATCH_SIZE}\"\n",
    "            )\n",
    "            if next_page_token:\n",
    "                url += f\"&pageToken={next_page_token}\"\n",
    "\n",
    "            data = None\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    response = self.session.get(url, timeout=10)\n",
    "                    response.raise_for_status()\n",
    "                    data = response.json()\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    page_info = f\"page {next_page_token}\" if next_page_token else \"first page\"\n",
    "                    print(f\"[WARN] CT.gov fetch fail on {page_info} (try {attempt+1}): {e}\")\n",
    "                    time.sleep(THROTTLE_SECONDS * 2)\n",
    "            if data is None:\n",
    "                break\n",
    "\n",
    "            studies = data.get(\"studies\", [])\n",
    "            if not studies:\n",
    "                break\n",
    "\n",
    "            for st in studies:\n",
    "                if len(kept_docs) >= RETMAX:\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    protocol = st.get(\"protocolSection\", {})\n",
    "                    nct = protocol.get(\"identificationModule\", {}).get(\"nctId\")\n",
    "                    if not nct:\n",
    "                        print(\"[WARN] Skipping study with missing nctId\")\n",
    "                        continue\n",
    "                    doc_id = f\"CT_{nct}\"\n",
    "                    if doc_id in self.ingested:\n",
    "                        continue\n",
    "\n",
    "                    title = (\n",
    "                        protocol.get(\"identificationModule\", {}).get(\"officialTitle\", \"\")\n",
    "                        or protocol.get(\"identificationModule\", {}).get(\"briefTitle\", \"\")\n",
    "                    )\n",
    "                    summary = protocol.get(\"descriptionModule\", {}).get(\"briefSummary\", \"\")\n",
    "                    details = protocol.get(\"descriptionModule\", {}).get(\"detailedDescription\", \"\")\n",
    "                    summary = _clean_html(summary)\n",
    "                    details = _clean_html(details)\n",
    "                    body_txt = (details or summary).strip()\n",
    "                    if not body_txt:\n",
    "                        continue\n",
    "\n",
    "                    start_date_str = protocol.get(\"statusModule\", {}).get(\"startDateStruct\", {}).get(\"date\", \"\")\n",
    "                    first_posted_str = protocol.get(\"statusModule\", {}).get(\"studyFirstPostDateStruct\", {}).get(\"date\", \"\")\n",
    "                    last_updated_str = protocol.get(\"statusModule\", {}).get(\"lastUpdatePostDateStruct\", {}).get(\"date\", \"\")\n",
    "                    \n",
    "                    start_date = _parse_date(start_date_str)\n",
    "                    first_posted = _parse_date(first_posted_str)\n",
    "                    last_updated = _parse_date(last_updated_str)\n",
    "\n",
    "                    if last_updated and last_updated.date() < self.cutoff.date():\n",
    "                        continue\n",
    "\n",
    "                    metadata = {\n",
    "                        \"status\": protocol.get(\"statusModule\", {}).get(\"overallStatus\", \"\"),\n",
    "                        \"phase\": protocol.get(\"designModule\", {}).get(\"phases\", []),\n",
    "                        \"study_type\": protocol.get(\"designModule\", {}).get(\"studyType\", \"\"),\n",
    "                        \"enrollment\": {\n",
    "                            \"count\": protocol.get(\"designModule\", {}).get(\"enrollmentInfo\", {}).get(\"count\", None),\n",
    "                            \"type\": protocol.get(\"designModule\", {}).get(\"enrollmentInfo\", {}).get(\"type\", \"\"),\n",
    "                        },\n",
    "                        \"eligibility\": {\n",
    "                            \"gender\": protocol.get(\"eligibilityModule\", {}).get(\"sex\", \"\"),\n",
    "                            \"min_age\": protocol.get(\"eligibilityModule\", {}).get(\"minimumAge\", \"\"),\n",
    "                            \"max_age\": protocol.get(\"eligibilityModule\", {}).get(\"maximumAge\", \"\"),\n",
    "                            \"healthy_vols\": protocol.get(\"eligibilityModule\", {}).get(\"healthyVolunteers\", False),\n",
    "                            \"criteria\": protocol.get(\"eligibilityModule\", {}).get(\"eligibilityCriteria\", \"\"),\n",
    "                        },\n",
    "                        \"conditions\": protocol.get(\"conditionsModule\", {}).get(\"conditions\", []),\n",
    "                        \"interventions\": [\n",
    "                            f\"{i.get('type', '')}:{i.get('name', '')}\" for i in protocol.get(\"armsInterventionsModule\", {}).get(\"interventions\", [])\n",
    "                        ],\n",
    "                        \"arms\": [\n",
    "                            {\n",
    "                                \"label\": arm.get(\"label\", \"\"),\n",
    "                                \"type\": arm.get(\"type\", \"\"),\n",
    "                                \"description\": arm.get(\"description\", \"\")\n",
    "                            } for arm in protocol.get(\"armsInterventionsModule\", {}).get(\"armGroups\", [])\n",
    "                        ],\n",
    "                        \"outcomes\": {\n",
    "                            \"primary\": [\n",
    "                                {\n",
    "                                    \"measure\": o.get(\"measure\", \"\"),\n",
    "                                    \"description\": o.get(\"description\", \"\")\n",
    "                                } for o in protocol.get(\"outcomesModule\", {}).get(\"primaryOutcomes\", [])\n",
    "                            ],\n",
    "                            \"secondary\": [\n",
    "                                {\n",
    "                                    \"measure\": o.get(\"measure\", \"\"),\n",
    "                                    \"description\": o.get(\"description\", \"\")\n",
    "                                } for o in protocol.get(\"outcomesModule\", {}).get(\"secondaryOutcomes\", [])\n",
    "                            ]\n",
    "                        },\n",
    "                        \"sponsors\": [protocol.get(\"sponsorCollaboratorsModule\", {}).get(\"leadSponsor\", {}).get(\"name\", \"\")],\n",
    "                        \"collaborators\": protocol.get(\"sponsorCollaboratorsModule\", {}).get(\"collaborators\", []),\n",
    "                        \"locations\": [\n",
    "                            {\n",
    "                                \"facility\": loc.get(\"facility\", \"\"),\n",
    "                                \"city\": loc.get(\"city\", \"\"),\n",
    "                                \"state\": loc.get(\"state\", \"\"),\n",
    "                                \"country\": loc.get(\"country\", \"\")\n",
    "                            } for loc in protocol.get(\"contactsLocationsModule\", {}).get(\"locations\", [])\n",
    "                        ]\n",
    "                    }\n",
    "                    \n",
    "                    published_date = None\n",
    "                    if last_updated:\n",
    "                        published_date = last_updated\n",
    "                    elif first_posted:\n",
    "                        published_date = first_posted\n",
    "                    elif start_date:\n",
    "                        published_date = start_date\n",
    "\n",
    "                    kept_docs.append({\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"source\": \"ClinicalTrials.gov\",\n",
    "                        \"doc_type\": \"clinical_trial\",\n",
    "                        \"title\": title,\n",
    "                        \"abstract\": summary,\n",
    "                        \"body_text\": body_txt,\n",
    "                        \"published_date\": published_date.isoformat(),\n",
    "                        \"published_date_ts\": 0.0 if not published_date else float(published_date.timestamp()),\n",
    "                        \"url\": f\"https://clinicaltrials.gov/study/{nct}\",\n",
    "                        \"metadata\": metadata\n",
    "                    })\n",
    "                    pbar.update(1)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Failed processing study {st.get('nct_id', 'unknown')}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            next_page_token = data.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "            time.sleep(THROTTLE_SECONDS)\n",
    "\n",
    "        pbar.close()\n",
    "        return kept_docs\n",
    "    \n",
    "    def fetch_news(self) -> list[dict[str, Any]]:\n",
    "        RETMAX_PER_FEED = 1000\n",
    "        THROTTLE_SECONDS = 1\n",
    "        MAX_RETRIES = 3\n",
    "        \n",
    "        FEEDS = {\n",
    "            \"Labiotech\": \"https://www.labiotech.eu/feed/\",\n",
    "            \"BioPharma Dive\": \"https://www.biopharmadive.com/feeds/news/\",\n",
    "            \"STAT News\": \"https://www.statnews.com/feed/\",\n",
    "            \"GEN News\": \"https://www.genengnews.com/feed/\",\n",
    "            \"Nature Biotechnology\": \"https://www.nature.com/nbt.rss\"\n",
    "        }\n",
    "\n",
    "        def _hash(text: str) -> str:\n",
    "            return hashlib.sha1(text.encode()).hexdigest()[:20]\n",
    "\n",
    "        def _get_entry_datetime(entry):\n",
    "            dt_struct = getattr(entry, \"published_parsed\", None) or getattr(entry, \"updated_parsed\", None)\n",
    "            if dt_struct:\n",
    "                return datetime.datetime(*dt_struct[:6])\n",
    "            return None\n",
    "\n",
    "        def _clean_html(html: str) -> str:\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            text = soup.get_text(separator=\" \")\n",
    "            return \" \".join(text.split())\n",
    "\n",
    "        def _looks_like_html_error(content: str) -> bool:\n",
    "            lowered = content.lower()\n",
    "            error_signals = [\n",
    "                \"attention required\",\n",
    "                \"cloudflare\",\n",
    "                \"sorry, you have been blocked\",\n",
    "                \"enable cookies\",\n",
    "                \"please turn javascript on\",\n",
    "                \"<html\",\n",
    "                \"<body\"\n",
    "            ]\n",
    "            if lowered.strip().startswith(\"<html\") and any(sig in lowered for sig in error_signals):\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        kept_docs: list[dict[str, Any]] = []\n",
    "\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                          \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                          \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "        for name, url in FEEDS.items():\n",
    "            successes = 0\n",
    "            feed = None\n",
    "\n",
    "            for attempt in range(1, MAX_RETRIES + 1):\n",
    "                try:\n",
    "                    resp = self.session.get(url, timeout=20)\n",
    "                    resp.raise_for_status()\n",
    "                    raw = resp.text\n",
    "\n",
    "                    if _looks_like_html_error(raw):\n",
    "                        print(f\"[WARN] Blocked or invalid content detected for feed {name} on attempt {attempt}\")\n",
    "                        time.sleep(THROTTLE_SECONDS * 3)\n",
    "                        continue\n",
    "\n",
    "                    feed = feedparser.parse(raw)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] RSS fetch {name} (try {attempt}/{MAX_RETRIES}): {e}\")\n",
    "                    time.sleep(THROTTLE_SECONDS * 2)\n",
    "\n",
    "            if not feed or not feed.entries:\n",
    "                print(f\"[WARN] No valid feed entries for {name}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            pbar = tqdm(total=RETMAX_PER_FEED, desc=f\"NEWS - {name:<15} processed\", unit=\"article\")\n",
    "\n",
    "            for entry in feed.entries:\n",
    "                if successes >= RETMAX_PER_FEED:\n",
    "                    break\n",
    "\n",
    "                pub_dt = _get_entry_datetime(entry)\n",
    "                if pub_dt is None or pub_dt.date() < self.cutoff.date():\n",
    "                    continue\n",
    "\n",
    "                link = entry.get(\"link\")\n",
    "                if not link:\n",
    "                    continue\n",
    "\n",
    "                guid = entry.get(\"id\") or link or entry.title\n",
    "                doc_id = f\"NEWS_{_hash(guid)}\"\n",
    "                if doc_id in self.ingested:\n",
    "                    continue\n",
    "\n",
    "                full_text = None\n",
    "                summary = None\n",
    "                for n in range(1, MAX_RETRIES + 1):\n",
    "                    try:\n",
    "                        art = Article(url=link, browser_user_agent=headers[\"User-Agent\"])\n",
    "                        art.download()\n",
    "                        art.parse()\n",
    "                        full_text = art.text or \"\"\n",
    "                        summary = art.meta_description or entry.get(\"summary\", \"\")\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"[WARN] Extraction failed for {link} (try {n}/{MAX_RETRIES}): {e}\")\n",
    "                        time.sleep(THROTTLE_SECONDS * 2)\n",
    "\n",
    "                summary = _clean_html(summary)\n",
    "\n",
    "                if not full_text:\n",
    "                    full_text = entry.get(\"summary\", \"\")\n",
    "                    summary = full_text\n",
    "\n",
    "                kept_docs.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"source\": name,\n",
    "                    \"doc_type\": \"news_article\",\n",
    "                    \"title\": entry.get(\"title\", \"\").strip(),\n",
    "                    \"abstract\": summary.strip() if summary else \"\",\n",
    "                    \"body_text\": full_text.strip() if full_text else \"\",\n",
    "                    \"published_date\": pub_dt.date().isoformat(),\n",
    "                    \"published_date_ts\": 0.0 if not pub_dt else float(pub_dt.timestamp()),\n",
    "                    \"url\": link,\n",
    "                    \"metadata\": {}\n",
    "                })\n",
    "                pbar.update(1)\n",
    "\n",
    "                successes += 1\n",
    "                time.sleep(THROTTLE_SECONDS)\n",
    "\n",
    "            pbar.close()\n",
    "            time.sleep(THROTTLE_SECONDS)\n",
    "\n",
    "        return kept_docs\n",
    "    \n",
    "    def run(self) -> list[dict[str, Any]]:\n",
    "        collected_docs = []\n",
    "        \n",
    "        try:\n",
    "            collected_docs.extend(self.fetch_pmc())\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] fetch_pmc failed: {e}\")\n",
    "\n",
    "        try:\n",
    "            collected_docs.extend(self.fetch_arxiv())\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] fetch_arxiv failed: {e}\")\n",
    "            \n",
    "        try:\n",
    "            collected_docs.extend(self.fetch_clinical_trials())\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] fetch_clinical_trials failed: {e}\")\n",
    "            \n",
    "        try:\n",
    "            collected_docs.extend(self.fetch_news())\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] fetch_news failed: {e}\")\n",
    "        \n",
    "        return collected_docs\n",
    "        \n",
    "    def _days_ago(self, n: int) -> datetime.datetime:\n",
    "        today = datetime.datetime.now(datetime.UTC).date()\n",
    "        return datetime.datetime.combine(today - datetime.timedelta(days=n), datetime.datetime.min.time())\n",
    "    \n",
    "    @staticmethod\n",
    "    def _clean_html(text: str) -> str:\n",
    "        soup = BeautifulSoup(text or \"\", \"html.parser\")\n",
    "        cleaned = soup.get_text(separator=\" \")\n",
    "        return \" \".join(cleaned.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestionModule():\n",
    "    def run(self, extracted_docs: list[dict[str, Any]]) -> set[str]:\n",
    "    \"\"\"\n",
    "    Ingests a list of extracted biomedical documents into the Qdrant vector database with hybrid search capabilities.\n",
    "\n",
    "    This method orchestrates the following workflow:\n",
    "        1. **Client and Collection Initialization:**  \n",
    "           Ensures a live connection to Qdrant and that the target collection and payload indexes exist. Handles connection retries.\n",
    "        2. **Document Preparation:**  \n",
    "           For each extracted document:\n",
    "               - Abstracts are wrapped into `Document` objects with relevant metadata.\n",
    "               - Full body text is chunked into overlapping segments (using the configured splitter). Each chunk is paired with metadata.\n",
    "        3. **Summarization and Keyword Extraction:**  \n",
    "           Each body chunk is summarized and annotated with up to 10 keywords using an LLM-based summarization chain.  \n",
    "           The output strictly follows a Python dictionary format suitable for further downstream parsing.\n",
    "           If the LLM fails or returns malformed output, the original chunk text is used as a fallback summary.\n",
    "        4. **Document Creation for Embedding:**  \n",
    "           All summary and abstract `Document` objects are prepared for embedding and vector storage.\n",
    "        5. **Batch Embedding and Storage:**  \n",
    "           All documents are embedded (dense and sparse) and persisted in Qdrant in batches, with error handling and exponential backoff on failures.\n",
    "           Source document IDs for all successfully stored docs are tracked.\n",
    "        6. **Callback Invocation:**  \n",
    "           Optionally, calls the provided `on_successful_ingest` callback with a set of source document IDs for each successful batch.\n",
    "\n",
    "    Args:\n",
    "        extracted_docs (list[dict[str, Any]]):  \n",
    "            List of documents as dictionaries, each representing extracted biomedical articles, clinical trials, or news.\n",
    "\n",
    "    Returns:\n",
    "        set[str]:  \n",
    "            Set of unique document IDs (`source_doc_id`) that were successfully stored in the vector database.\n",
    "\n",
    "    Notes:\n",
    "        - Handles batching for both summarization and embedding to optimize throughput and reliability.\n",
    "        - Uses retry with exponential backoff for LLM summarization and Qdrant storage to handle transient failures.\n",
    "        - Enforces all output summaries and keywords conform to a strict, parseable Python dictionary format for downstream processing.\n",
    "        - Provides progress updates via `tqdm` and logs key events, errors, and warnings to the console.\n",
    "        - Supports hybrid search by storing both dense and sparse embeddings.\n",
    "        - Designed to support millions of documents with scalable chunking and batching.\n",
    "\n",
    "    Raises:\n",
    "        ConnectionError: If unable to connect to Qdrant after all retries.\n",
    "\n",
    "    Example:\n",
    "        >>> ingestion = DataIngestionModule(qdrant_url, qdrant_api_key, google_api_key)\n",
    "        >>> stored_ids = ingestion.run(extracted_docs)\n",
    "        >>> print(f\"{len(stored_ids)} documents ingested.\")\n",
    "    \"\"\"\n",
    "\n",
    "    SUMMARY_PROMPT_TEMPLATE = \"\"\"You are a highly specialized text processing AI. Your sole task is to analyze the provided bio-medical text and generate a Python dictionary string.\n",
    "\n",
    "Follow these instructions METICULOUSLY:\n",
    "\n",
    "1.  **Output Format:** Your response MUST be a single, valid Python dictionary string.\n",
    "    The dictionary must conform EXACTLY to this structure:\n",
    "    `{{ 'summary': \"<summary_string>\", 'keywords': [\"<keyword1>\", \"<keyword2>\", ...] }}`\n",
    "\n",
    "2.  **Content for 'summary' key:**\n",
    "    * The value must be a concise and comprehensive summary of the input text.\n",
    "    * Focus on the most critical bio-medical information, key findings, and essential details.\n",
    "    * The summary MUST be strictly 200 words or less.\n",
    "    * It must be a single string.\n",
    "\n",
    "3.  **Content for 'keywords' key:**\n",
    "    * The value must be a Python list of strings.\n",
    "    * Extract atmost 10 of the most relevant and specific bio-medical keywords or keyphrases from the text.\n",
    "    * These keywords should be suitable for use in a hybrid search system (meaning they should be significant terms, entities, or concepts).\n",
    "    * Each item in the list must be a string.\n",
    "\n",
    "4.  **Example of PERFECT output format:**\n",
    "    `{{ 'summary': 'The study investigates the effect of Compound X on murine models of Alzheimer's disease. Results indicate a significant reduction in amyloid-beta plaques and improved cognitive function compared to placebo. No adverse effects were reported at the tested dosage.', 'keywords': ['Compound X', 'Alzheimer's disease', 'murine models', 'amyloid-beta plaques', 'cognitive function', 'placebo'] }}`\n",
    "\n",
    "5.  **CRITICAL - Do NOT:**\n",
    "    * Do NOT include any text, explanation, apologies, or conversational filler before or after the Python dictionary string.\n",
    "    * Do NOT use markdown (e.g., ```python ... ``` or ```json ... ```) to wrap your output.\n",
    "    * Do NOT deviate from the specified dictionary structure or key names.\n",
    "    * Ensure all strings within the dictionary (keys and values) are properly quoted using single quotes for the outer dictionary and keys, and single or double quotes for string values as per valid Python syntax. Double check your quote usage to ensure the output is a valid Python dictionary string.\n",
    "\n",
    "Text to process:\n",
    "{text}\n",
    "\n",
    "Your Python dictionary string output:\n",
    "\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        qdrant_url: str,\n",
    "        qdrant_api_key: Union[SecretStr, str],\n",
    "        google_api_key: Union[SecretStr, str],\n",
    "        summary_model: str = \"gemini-1.5-flash-8b\",\n",
    "        embed_model: str = \"models/embedding-001\",\n",
    "        chunk_size: int = 2000,\n",
    "        chunk_overlap: int = 200,\n",
    "        base_dir: Path = Path(\"./project_asclepius\"),\n",
    "        qdrant_dir: Path = Path(\"./qdrant_db\"),\n",
    "        on_successful_ingest: Optional[Callable[[set[str]], None]] = None\n",
    "    ):\n",
    "        base_dir.mkdir(exist_ok=True)\n",
    "        # qdrant_dir = base_dir / qdrant_dir if not qdrant_dir.is_absolute() else qdrant_dir\n",
    "        # qdrant_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        if isinstance(qdrant_api_key, SecretStr):\n",
    "            qdrant_api_key = qdrant_api_key.get_secret_value()\n",
    "            \n",
    "        self.qdrant_api_key = qdrant_api_key\n",
    "        self.qdrant_url = qdrant_url\n",
    "        \n",
    "        if isinstance(google_api_key, str):\n",
    "            google_api_key = SecretStr(google_api_key)\n",
    "            \n",
    "        self.google_api_key = google_api_key\n",
    "        \n",
    "        self.on_successful_ingest = on_successful_ingest\n",
    "        \n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = chunk_size, chunk_overlap = chunk_overlap\n",
    "        )\n",
    "        self.embedding_model = GoogleGenerativeAIEmbeddings(model=embed_model, google_api_key=google_api_key)\n",
    "        self.sparse_model = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "        \n",
    "        self.qdrant_dir = qdrant_dir\n",
    "        self.collection_name = \"project_asclepius\"\n",
    "        \n",
    "        self.summary_generation_agent = ChatGoogleGenerativeAI(\n",
    "            model=summary_model,\n",
    "            api_key=google_api_key,\n",
    "            temperature=0,\n",
    "            max_tokens=2048 # crucial because we are dealing with lots of data and there is a good chance that without this restriction -> LLM can generate really long summaries for atleast few of the data\n",
    "        )\n",
    "        self.summary_generation_prompt_template = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=self.SUMMARY_PROMPT_TEMPLATE\n",
    "        )\n",
    "        self.summary_generation_chain = (\n",
    "            self.summary_generation_prompt_template\n",
    "            | self.summary_generation_agent\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "    def run(self, extracted_docs: list[dict[str, Any]]) -> set[str]:\n",
    "        SUMMARY_BATCH_SIZE = 100\n",
    "        EMBEDDING_BATCH_SIZE = 100\n",
    "        MAX_RETRIES = 5\n",
    "        RETRY_BACKOFF_SECS = 5\n",
    "        \n",
    "        if not extracted_docs:\n",
    "            return set()\n",
    "        \n",
    "        try:\n",
    "            client = self._get_client(self.qdrant_url, self.qdrant_api_key)\n",
    "            self._ensure_collection_exists(self.qdrant_url, self.qdrant_api_key)\n",
    "        except ConnectionError as e:\n",
    "            print(f\"[ERROR] Cannot connect to Qdrant at {self.qdrant_url}: {e}\")\n",
    "            return set()\n",
    "        \n",
    "        self.vectorstore = QdrantVectorStore(\n",
    "            client=client,\n",
    "            collection_name=self.collection_name,\n",
    "            embedding=self.embedding_model,\n",
    "            sparse_embedding=self.sparse_model,\n",
    "            retrieval_mode=RetrievalMode.HYBRID,\n",
    "            vector_name=\"dense\",\n",
    "            sparse_vector_name=\"langchain-sparse\"\n",
    "        )\n",
    "        \n",
    "        abstract_docs: list[Document] = []\n",
    "        chunk_records: list[dict[str, Any]] = []\n",
    "            \n",
    "        for doc in extracted_docs:\n",
    "            abstract = doc.get(\"abstract\", doc.get(\"title\", \"\")).strip()\n",
    "            # if there is no abstract -> based on `title`/`body_text` you can use an agent to generate abstracts\n",
    "            if abstract:\n",
    "                meta = {\n",
    "                    \"published_date\": doc.get(\"published_date\") or \"\",\n",
    "                    \"published_date_ts\": doc.get(\"published_date_ts\") or 0.0,\n",
    "                    \"content_type\": \"abstract\",\n",
    "                    \"source\": doc.get(\"source\") or \"\",\n",
    "                    \"source_doc_id\": doc.get(\"doc_id\") or \"\",\n",
    "                    \"url\": doc.get(\"url\") or \"\",\n",
    "                    \"title\": doc.get(\"title\") or \"\",\n",
    "                    \"original_doc_type\": doc.get(\"doc_type\") or \"\"\n",
    "                }\n",
    "                abstract_docs.append(Document(page_content=abstract, metadata=meta))\n",
    "\n",
    "            body = doc.get(\"body_text\", \"\").strip()\n",
    "            if not body:\n",
    "                continue\n",
    "\n",
    "            chunks = self.text_splitter.split_text(body)\n",
    "            for idx, chunk in enumerate(chunks):\n",
    "                meta = {\n",
    "                    \"published_date\": doc.get(\"published_date\") or \"\",\n",
    "                    \"published_date_ts\": doc.get(\"published_date_ts\") or 0.0,\n",
    "                    \"content_type\": \"chunk\",\n",
    "                    \"chunk_index\": idx,\n",
    "                    \"source\": doc.get(\"source\") or \"\",\n",
    "                    \"source_doc_id\": doc.get(\"doc_id\") or \"\",\n",
    "                    \"url\": doc.get(\"url\") or \"\",\n",
    "                    \"title\": doc.get(\"title\") or \"\",\n",
    "                    \"original_doc_type\": doc.get(\"doc_type\") or \"\"\n",
    "                }\n",
    "                chunk_records.append({\"text\": chunk, \"meta\": meta})\n",
    "                \n",
    "        if chunk_records:\n",
    "            print(f\"[INFO] Summarizing {len(chunk_records):,} chunks…\")\n",
    "                \n",
    "        chunk_summaries: list[dict[str, Any]] = [{} for _ in chunk_records]\n",
    "        for start in tqdm(range(0, len(chunk_records), SUMMARY_BATCH_SIZE), unit=\"batch\", desc=\"Batches summarized\"):\n",
    "            end = start + SUMMARY_BATCH_SIZE\n",
    "            batch = chunk_records[start:end]\n",
    "            batch_inputs = [{\"text\": rec[\"text\"]} for rec in batch]\n",
    "            \n",
    "            for attempt in range(1, MAX_RETRIES + 1):\n",
    "                try:\n",
    "                    outputs: list[str] = self.summary_generation_chain.batch(batch_inputs)\n",
    "                    if len(outputs) != len(batch):\n",
    "                        raise RuntimeError(\"LLM returned unexpected number of summaries.\")\n",
    "                    \n",
    "                    parsed_outputs: list[dict[str, Any]] = []\n",
    "                    for i, output_str in enumerate(outputs):\n",
    "                        try:\n",
    "                            data = ast.literal_eval(output_str)\n",
    "                            if not (\n",
    "                                isinstance(data, dict) and\n",
    "                                isinstance(data.get(\"summary\"), str) and\n",
    "                                isinstance(data.get(\"keywords\"), list) and\n",
    "                                all(isinstance(kw, str) for kw in data[\"keywords\"])\n",
    "                            ):\n",
    "                                raise ValueError()\n",
    "                            parsed_outputs.append(data)\n",
    "                        except Exception:\n",
    "                            parsed_outputs.append({\"summary\": batch_inputs[i][\"text\"], \"keywords\": []})\n",
    "                    \n",
    "                    chunk_summaries[start:end] = parsed_outputs\n",
    "                    time.sleep(2) # for avoding LangChain internal server errors\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt == MAX_RETRIES:\n",
    "                        print(f\"[WARN] Final failure summarizing batch {start}-{end}: {e}\")\n",
    "                    else:\n",
    "                        wait = RETRY_BACKOFF_SECS * (2 ** (attempt - 1))\n",
    "                        print(\n",
    "                            f\"[WARN] Summary batch {start}-{end} failed ({e}). \"\n",
    "                            f\"Retrying in {wait}s…\"\n",
    "                        )\n",
    "                        time.sleep(wait)\n",
    "                        \n",
    "            # Fallback to storing original chunk text if summary generation failed\n",
    "            for i in range(start, min(end, len(chunk_summaries))):\n",
    "                if not chunk_summaries[i]:\n",
    "                    chunk_summaries[i] = {\"summary\": chunk_records[i][\"text\"], \"keywords\": []}\n",
    "                        \n",
    "        chunk_docs: list[Document] = []\n",
    "        for rec, parsed in zip(chunk_records, chunk_summaries):\n",
    "            summary_text = parsed[\"summary\"]\n",
    "            keywords = parsed[\"keywords\"]\n",
    "            meta = rec[\"meta\"]\n",
    "            meta[\"original_content\"] = rec[\"text\"]\n",
    "            meta[\"keywords\"] = keywords\n",
    "            chunk_docs.append(Document(page_content=summary_text, metadata=meta))\n",
    "            \n",
    "        all_docs = abstract_docs + chunk_docs\n",
    "        stored_source_ids: set[str] = set()\n",
    "        stored_doc_count = 0\n",
    "        \n",
    "        if all_docs:\n",
    "            print(f\"[INFO] Persisting {len(all_docs):,} docs to Qdrant…\")\n",
    "            \n",
    "        for start in tqdm(range(0, len(all_docs), EMBEDDING_BATCH_SIZE), unit=\"batch\", desc=\"Batches stored\"):\n",
    "            end = start + EMBEDDING_BATCH_SIZE\n",
    "            batch_docs = all_docs[start:end]\n",
    "\n",
    "            for attempt in range(1, MAX_RETRIES + 1):\n",
    "                try:\n",
    "                    self.vectorstore.add_documents(batch_docs)\n",
    "                    batch_ids: set[str] = set()\n",
    "                    for d in batch_docs:\n",
    "                        stored_source_ids.add(d.metadata[\"source_doc_id\"])\n",
    "                        batch_ids.add(d.metadata[\"source_doc_id\"])\n",
    "                    stored_doc_count += len(batch_docs)\n",
    "                    if self.on_successful_ingest:\n",
    "                        self.on_successful_ingest(batch_ids)\n",
    "                    time.sleep(5) # for avoiding 429 ResourceExhaust erors\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt == MAX_RETRIES:\n",
    "                        print(f\"[WARN] Final failure embedding batch {start}-{end}: {e}\")\n",
    "                    else:\n",
    "                        wait = RETRY_BACKOFF_SECS * (2 ** (attempt - 1))\n",
    "                        print(\n",
    "                            f\"[WARN] Embedding batch {start}-{end} failed ({e}). \"\n",
    "                            f\"Retrying in {wait}s…\"\n",
    "                        )\n",
    "                        time.sleep(wait)\n",
    "                        \n",
    "        print(f\"[INFO] Ingestion complete · {stored_doc_count:,} items belonging to {len(stored_source_ids):,} source docs stored.\")\n",
    "        return stored_source_ids\n",
    "    \n",
    "    def _ensure_collection_exists(self, qdrant_url: str, qdrant_api_key: str):\n",
    "        client = self._get_client(qdrant_url=qdrant_url, qdrant_api_key=qdrant_api_key)\n",
    "        try:\n",
    "            client.get_collection(self.collection_name)\n",
    "        except Exception:\n",
    "            print(f\"[INFO] Creating collection '{self.collection_name}'...\")\n",
    "            sample_embedding = self.embedding_model.embed_query(\"test\")\n",
    "            embedding_dim = len(sample_embedding)\n",
    "            \n",
    "            client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config={\n",
    "                    \"dense\": VectorParams(\n",
    "                        size=embedding_dim,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                },\n",
    "                sparse_vectors_config={\n",
    "                    \"langchain-sparse\": SparseVectorParams(\n",
    "                        index=models.SparseIndexParams(on_disk=False)\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "            print(f\"[SUCCESS] Collection created with dense dimension {embedding_dim} and sparse vectors.\")\n",
    "            \n",
    "        self._ensure_payload_indexes(client)\n",
    "            \n",
    "    def _ensure_payload_indexes(self, client: QdrantClient):\n",
    "        required_indexes = [\n",
    "            (\"metadata.content_type\", \"keyword\"),\n",
    "            (\"metadata.published_date_ts\", \"float\"),\n",
    "            (\"metadata.original_doc_type\", \"keyword\"), \n",
    "            (\"metadata.source\", \"keyword\"),\n",
    "            (\"metadata.source_doc_id\", \"keyword\"),\n",
    "            (\"metadata.url\", \"keyword\"),\n",
    "            (\"metadata.chunk_index\", \"integer\"),\n",
    "            (\"metadata.title\", \"keyword\"),\n",
    "            (\"metadata.keywords\", \"keyword\"),\n",
    "            (\"metadata.original_content\", \"text\")\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            collection_info = client.get_collection(self.collection_name)\n",
    "            existing_indexes = set(collection_info.payload_schema.keys())\n",
    "        except:\n",
    "            existing_indexes = set()\n",
    "        \n",
    "        for field_name, field_type in required_indexes:\n",
    "            if field_name not in existing_indexes:\n",
    "                try:\n",
    "                    client.create_payload_index(\n",
    "                        collection_name=self.collection_name,\n",
    "                        field_name=field_name,\n",
    "                        field_schema=field_type\n",
    "                    )\n",
    "                    print(f\"[DataIngestPipeline] Successfully created index for {field_name}\")\n",
    "                except Exception as e:\n",
    "                    if \"already exists\" in str(e).lower():\n",
    "                        print(f\"[DataIngestPipeline] Index for {field_name} already exists\")\n",
    "                    else:\n",
    "                        print(f\"[DataIngestPipeline] Warning: Could not create index for {field_name}: {e}\")\n",
    "            \n",
    "    def _get_client(self, qdrant_url: str, qdrant_api_key: str):\n",
    "#         if not hasattr(self, '_client') or self._client is None:\n",
    "#             lock_file = os.path.join(str(self.qdrant_dir), \".lock\")\n",
    "#             if os.path.exists(lock_file):\n",
    "#                 try:\n",
    "#                     os.remove(lock_file)\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"[WARN] Could not remove lock file: {e}\")\n",
    "            \n",
    "#             self._client = QdrantClient(path=str(self.qdrant_dir))\n",
    "        MAX_RETRIES = 3\n",
    "        BACKOFF = 4\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)\n",
    "                # quick health check\n",
    "                client.get_collections()\n",
    "                self._client = client\n",
    "                return client\n",
    "            except Exception as exc:\n",
    "                print(f\"[WARN] Qdrant connection attempt {attempt} failed: {exc}\")\n",
    "                if attempt >= MAX_RETRIES:\n",
    "                    raise ConnectionError(\"All Qdrant connection attempts failed.\") from exc\n",
    "                time.sleep(BACKOFF)\n",
    "                BACKOFF *= 2\n",
    "            \n",
    "    def close(self):\n",
    "        if hasattr(self, '_client') and self._client is not None:\n",
    "            try:\n",
    "                self._client.close()\n",
    "                print(\"[INFO] QdrantClient closed successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Error closing client: {e}\")\n",
    "            finally:\n",
    "                self._client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902cfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestPipeline():\n",
    "    \"\"\"\n",
    "    Executes the full biomedical data extraction and ingestion pipeline.\n",
    "\n",
    "    This method coordinates the end-to-end workflow for collecting, processing, \n",
    "    summarizing, and storing biomedical documents in a Qdrant vector database. \n",
    "    It performs the following sequence:\n",
    "\n",
    "        1. **Extraction:**  \n",
    "           Uses the `DataExtractionModule` to fetch new documents from various biomedical sources \n",
    "           (e.g., PubMed Central, arXiv, ClinicalTrials.gov, news feeds), \n",
    "           excluding any documents already present in the ingest log.\n",
    "\n",
    "        2. **Ingestion:**  \n",
    "           Uses the `DataIngestionModule` to:\n",
    "               - Summarize and keyword-tag each document (and its body text chunks) via LLM.\n",
    "               - Embed the resulting texts (dense + sparse) for hybrid search.\n",
    "               - Persist all items into Qdrant, while updating the log of ingested document IDs.\n",
    "\n",
    "        3. **Progress Reporting:**  \n",
    "           Prints progress and completion messages to the console.\n",
    "\n",
    "        4. **Resource Cleanup:**  \n",
    "           Ensures the underlying Qdrant client connection is closed, regardless of success or errors.\n",
    "\n",
    "    Notes:\n",
    "        - New document IDs are immediately added to the ingest log after successful storage,\n",
    "          preventing redundant re-processing in future pipeline runs.\n",
    "        - This pipeline is designed for periodic or scheduled operation in a production setting.\n",
    "\n",
    "    Raises:\n",
    "        Any exceptions from extraction or ingestion modules are allowed to propagate after \n",
    "        cleanup and logging.\n",
    "\n",
    "    Example:\n",
    "        >>> pipeline = DataIngestPipeline(qdrant_url, qdrant_api_key, ncbi_api_key, google_api_key)\n",
    "        >>> pipeline.run()\n",
    "        [INFO] Starting data extraction...\n",
    "        [INFO] Extraction complete. 3,500 docs fetched for ingestion.\n",
    "        [INFO] Starting ingestion to Qdrant...\n",
    "        [INFO] Pipeline finished · 2,770 new source docs added.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        qdrant_url: str,\n",
    "        qdrant_api_key: Union[SecretStr, str],\n",
    "        ncbi_api_key: Union[SecretStr, str],\n",
    "        google_api_key: Union[SecretStr, str],\n",
    "        lookup_window_size: int = 365 * 3, # (in days)\n",
    "        ingest_log_path: Path = Path(\"./ingested.json\"),\n",
    "        summary_model: str = \"gemini-1.5-flash-8b\",\n",
    "        embed_model: str = \"models/embedding-001\",\n",
    "        chunk_size: int = 2000,\n",
    "        chunk_overlap: int = 200,\n",
    "        base_dir: Path = Path(\"./project_asclepius\"),\n",
    "        qdrant_dir: Path = Path(\"./qdrant_db\")\n",
    "    ):\n",
    "        base_dir.mkdir(exist_ok=True)\n",
    "        ingest_log_path = base_dir / ingest_log_path if not ingest_log_path.is_absolute() else ingest_log_path\n",
    "        \n",
    "        if not qdrant_url:\n",
    "            print(\"[ERROR] Qdrant URL is required but was not provided.\")\n",
    "            raise ValueError(\"Qdrant URL is required.\")\n",
    "        if not qdrant_api_key:\n",
    "            print(\"[ERROR] Qdrant API key is required but was not provided.\")\n",
    "            raise ValueError(\"Qdrant API key is required.\")\n",
    "        \n",
    "        if isinstance(qdrant_api_key, SecretStr):\n",
    "            qdrant_api_key = qdrant_api_key.get_secret_value()\n",
    "        \n",
    "        self.qdrant_url = qdrant_url\n",
    "        self.qdrant_api_key = qdrant_api_key\n",
    "        \n",
    "        self.ingest_log_path = ingest_log_path\n",
    "        self.ingested: set[str] = self._load_ingest_log()\n",
    "        \n",
    "        self.data_extraction_module = DataExtractionModule(\n",
    "            ncbi_api_key=ncbi_api_key,\n",
    "            ingested_logs=self.ingested,\n",
    "            lookup_window_size=lookup_window_size\n",
    "        )\n",
    "        \n",
    "        self.data_ingestion_module = DataIngestionModule(\n",
    "            qdrant_url=self.qdrant_url,\n",
    "            qdrant_api_key=self.qdrant_api_key,\n",
    "            google_api_key=google_api_key,\n",
    "            summary_model=summary_model,\n",
    "            embed_model=embed_model,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            base_dir=base_dir,\n",
    "            qdrant_dir=qdrant_dir,\n",
    "            on_successful_ingest=self._eagerly_update_log\n",
    "        )\n",
    "        \n",
    "    def run(self):\n",
    "        try:\n",
    "            print(\"[INFO] Starting data extraction...\")\n",
    "            extracted_docs = self.data_extraction_module.run()\n",
    "            print(f\"[INFO] Extraction complete. {len(extracted_docs):,} docs fetched for ingestion.\")\n",
    "            print(\"[INFO] Starting ingestion to Qdrant...\")\n",
    "            stored_ids = self.data_ingestion_module.run(extracted_docs)\n",
    "            print(f\"[INFO] Pipeline finished · {len(stored_ids):,} new source docs added.\")\n",
    "        finally:\n",
    "            self.data_ingestion_module.close()\n",
    "        \n",
    "    def _load_ingest_log(self) -> set[str]:\n",
    "        if self.ingest_log_path.exists():\n",
    "            try:\n",
    "                text = self.ingest_log_path.read_text(encoding=\"utf-8\")\n",
    "                data = json.loads(text)\n",
    "                if isinstance(data, list):\n",
    "                    return set(str(x) for x in data)\n",
    "                else:\n",
    "                    print(\"[WARN] Ingest log is not a list. Starting with empty log.\")\n",
    "                    return set()\n",
    "            except (json.JSONDecodeError, UnicodeDecodeError) as e:\n",
    "                print(f\"[WARN] Ingest log corrupted or not valid JSON: {e}. Starting with empty log.\")\n",
    "                return set()\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not read ingest log: {e}. Starting with empty log.\")\n",
    "                return set()\n",
    "        else:\n",
    "            return set()\n",
    "        \n",
    "    def _save_ingest_log(self):\n",
    "        try:\n",
    "            self.ingest_log_path.write_text(json.dumps(sorted(self.ingested)), encoding=\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to save ingest log: {e}\")\n",
    "            \n",
    "    def _eagerly_update_log(self, new_ids: set[str]):\n",
    "        self.ingested.update(new_ids)\n",
    "        self._save_ingest_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c47e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run data ingestion pipeline\n",
    "\n",
    "# use your own keys\n",
    "qdrant_url=\"\"\n",
    "qdrant_api_key=\"\"\n",
    "ncbi_api_key=\"\"\n",
    "google_api_key=\"\"\n",
    "\n",
    "obj = DataIngestPipeline(\n",
    "    qdrant_url=qdrant_url,\n",
    "    qdrant_api_key=qdrant_api_key,\n",
    "    ncbi_api_key=ncbi_api_key,\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "\n",
    "res = obj.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
